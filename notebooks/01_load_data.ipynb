{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bcc8223",
   "metadata": {},
   "source": [
    "# 1. Data Exploration & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e8c66",
   "metadata": {},
   "source": [
    "## Import dependencies, set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96f109",
   "metadata": {},
   "source": [
    "1. `python3 -m venv .venv`\n",
    "2. utilize virtual environment\n",
    "    - (LINUX/MAC) `source .venv/bin/activate`\n",
    "    - (WINDOWS) `.venv\\Scripts\\Activate.    ps1`\n",
    "3. `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e55a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter magic\n",
    "%run ../util/dependencies.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98389ba",
   "metadata": {},
   "source": [
    "## Planetary Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddeb4ae",
   "metadata": {},
   "source": [
    "Upon reviewing the quality of data from the \"Planetary Systems\" (PS) database, it was deemed better to pivot towards the \"Planetary Systems Composite Data\" (PSCompPars) database. In brief:\n",
    "1. PS details a record for each exoplanet and each one of its references (this helps us reach original literature analyses of these bodies). Missing data is prevelant.\n",
    "2. PSCompPars curates a “best available” or “most complete” set of parameters for each planet, pulling from multiple references.\n",
    "\n",
    "So far as our exploration of exoplanets and their stars (studying the whole population of exoplanets thus far), this seems outside the scope of our analysis, and creates a cumbersome process of exploring the data. This will aid in limiting time spent cleaning the dataset, and limit our analysis to 6065 from a daunting ~32,000 records.\n",
    "\n",
    "For an explanation on how the composite dataset aggregates all available information on exoplanet figures, please see <https://exoplanetarchive.ipac.caltech.edu/docs/pscp_calc.html>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da9c128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully. Number of records: 6065\n",
      "Data saved as 'pscp_01_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "# TAP base URL (Planetary Systems Composite Data)\n",
    "url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n",
    "# The \"Planetary Systems Composite Data\" database (confirmed exoplanets) is encoded as \"PSCompPars\" within the Exoplanet Archive\n",
    "ADQL_query =    \"SELECT \" \\\n",
    "                \"pl_controv_flag, pl_name, hostname, pl_letter, sy_snum, discoverymethod, disc_year,\" \\\n",
    "                \"pl_radj, pl_massj, st_spectype, st_rad, st_mass, st_met, st_lum, st_teff, st_rad \" \\\n",
    "                \"FROM PSCompPars\"\n",
    "\n",
    "# Request data as CSV\n",
    "params = {\n",
    "    \"query\": ADQL_query,\n",
    "    \"format\": \"csv\"\n",
    "}\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Load into \"Planetary Systems\" DataFrame\n",
    "pscp = pd.read_csv(io.StringIO(response.text))\n",
    "print(\"Data loaded successfully. Number of records:\", len(pscp))\n",
    "\n",
    "# Save to parquet for local use and following notebooks\n",
    "# (parquets are smaller files than csv, so good as an intermediate file type for future processing)\n",
    "file_name = 'pscp_01_raw.csv'\n",
    "\n",
    "try:\n",
    "    pscp.to_csv('../data/' + file_name, index=False)\n",
    "    print('Data saved as \\'' + file_name + '\\'')\n",
    "except Exception as e:\n",
    "    print('Data failed to save as \\'' + file_name + '\\': ' + e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab8fb0",
   "metadata": {},
   "source": [
    "Over 355 columns in this dataset!! All different features we can analyze in another related project pertaining to exoplanet exploration and methods for doing so . . .\n",
    "\n",
    "After reviewing the column descriptions (as defined here: <https://exoplanetarchive.ipac.caltech.edu/docs/API_PS_columns.html>), the following features are relevant to our exploration:\n",
    "1. pl_controv_flag (is the comfirmation of this planet questioned?)\n",
    "2. pl_name\n",
    "3. hostname (most common star name)\n",
    "4. pl_letter\n",
    "5. sy_snum\n",
    "6. discoverymethod\n",
    "7. disc_year\n",
    "8. pl_radj\n",
    "9. pl_massj\n",
    "10. st_spectype\n",
    "11. st_rad\n",
    "12. st_mass\n",
    "13. st_met\n",
    "14. st_lum\n",
    "15. st_rad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251e312",
   "metadata": {},
   "source": [
    "As we will see it later, it is important to get an overview of all of the stars we have seen in the observable universe thus far, to draw a picture of what exoplanet host (stellar) date is accessable to us, vs the stars that are in the observable universe. This is done as a means to detect bias from:\n",
    "1. **Educated Assumptions** Stars we choose to observe,\n",
    "2. **Technical Limitations** Stars are easier to observe, and\n",
    "3. **Physical Stellar Characteristics** Stars that tend to have more planets\n",
    "\n",
    "As such, the following dataset was appended to the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84284b41",
   "metadata": {},
   "source": [
    "## Kepler Stellar\n",
    "Observe the data from the kepler mission, designed to identify exoplanets in our observable universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5578361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully. Number of records: 990244\n",
      "Data saved as 'ks_01_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "# TAP base URL (Kepler Stellar Table)\n",
    "url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n",
    "# The \"Kepler Stellar Table\" database (confirmed exoplanets) is encoded as \"keplerstellar\" within the Exoplanet Archive\n",
    "ADQL_query =    \"SELECT \" \\\n",
    "                \"kepid, tm_designation, teff, feh, radius, mass, dens, \" \\\n",
    "                \"nconfp, nkoi, ntce \" \\\n",
    "                \"FROM keplerstellar\"\n",
    "\n",
    "# Request data as CSV\n",
    "params = {\n",
    "    \"query\": ADQL_query,\n",
    "    \"format\": \"csv\"\n",
    "}\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Load into \"Kepler Stellar\" DataFrame\n",
    "ks = pd.read_csv(io.StringIO(response.text))\n",
    "print(\"Data loaded successfully. Number of records:\", len(ks))\n",
    "\n",
    "# Save raw data as csv\n",
    "file_name = 'ks_01_raw.csv'\n",
    "\n",
    "try:\n",
    "    ks.to_csv('../data/' + file_name, index=False)\n",
    "    print('Data saved as \\'' + file_name + '\\'')\n",
    "except Exception as e:\n",
    "    print('Data failed to save as \\'' + file_name + '\\': ' + e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae3abcd",
   "metadata": {},
   "source": [
    "## Gaia DR3\n",
    "This dataset is utilized to determine the makeup of stars in the (known) universe. While the distribution of star classifications has already been done (and referenced in the README), this data was explored for the following reasons:\n",
    "1. Practice utilizing APIs in a python context\n",
    "2. Experience utilizing the the Gaia DR3 dataset: data I am sure to revisit in future astro data analytic endevours\n",
    "3. Verifying 3rd party figures are accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "791e7234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n"
     ]
    }
   ],
   "source": [
    "from astroquery.gaia import Gaia\n",
    "\n",
    "# Queary for retrieving the stellar \"classification\" data\n",
    "# This query utilizes random_index to achieve an unbiased representation of the population of observed stars\n",
    "OLD_ADQL_QUERY = \"\"\"\n",
    "SELECT\n",
    "    gs.source_id,\n",
    "    gs.teff_gspphot,\n",
    "    gs.bp_rp,\n",
    "    gs.random_index,\n",
    "    ap.radius_gspphot,\n",
    "    ap.mass_flame\n",
    "FROM gaiadr3.gaia_source AS gs\n",
    "INNER JOIN gaiadr3.astrophysical_parameters as ap\n",
    "ON gs.source_id = ap.source_id\n",
    "WHERE\n",
    "    gs.teff_gspphot IS NOT NULL\n",
    "    AND gs.parallax > 0\n",
    "    AND gs.random_index < 20000\n",
    "\"\"\"\n",
    "\n",
    "# made a mistake of limiting to 4806 records with INNER JOIN, thus . . .\n",
    "ADQL_QUERY = \"\"\"\n",
    "SELECT\n",
    "    gs.source_id,\n",
    "    gs.teff_gspphot,\n",
    "    gs.bp_rp,\n",
    "    gs.random_index,\n",
    "    ap.radius_gspphot,\n",
    "    ap.mass_flame\n",
    "FROM gaiadr3.gaia_source AS gs\n",
    "LEFT JOIN gaiadr3.astrophysical_parameters as ap\n",
    "ON gs.source_id = ap.source_id\n",
    "WHERE\n",
    "    gs.parallax > 0\n",
    "    AND gs.random_index < 20000000 --twenty million stars!!\n",
    "\"\"\"\n",
    "\n",
    "# Run the query\n",
    "Gaia.MAIN_GAIA_TABLE = \"gaiadr3.gaia_source\"\n",
    "Gaia.ROW_LIMIT = -1  # no limit\n",
    "my_job_query = Gaia.launch_job_async(ADQL_QUERY) # async for long queries?!!?\n",
    "gaia_df = my_job_query.get_results().to_pandas()\n",
    "\n",
    "# Save in CSV file for further analysis in '02_clean_and\\ _explore_gaiadr3.ipynb'\n",
    "file_name = 'gaiadr3_01_raw'\n",
    "gaia_df.to_csv('../data/' + file_name + '.csv', index_label=False) # accidentally included indeces originally :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac1c4f4",
   "metadata": {},
   "source": [
    "### 3 mil Cap, is this sample sufficient?\n",
    "Turns out that I needed to rewrite the following line: \"my_job_query = Gaia.launch_job_async(ADQL_QUERY)\", as Gaia limits (on the server side) query responses to 2000 records for synced queries! Good to know . . .\n",
    "\n",
    "It seems like there is a row limit of 3 million, I tried to fix this by having a ROW_LIMIT of -1, but ultimately I will be limited by what the server allows. Further investigation sees that I have hit the threshold of 3 million returned records by Gaia's servers, and unfortunatly this is a hard cap to avoid potential issues with overloading their servers.\n",
    "\n",
    "Alternatively, I could work around this feature by doing chunks of calls to the server, and then combining this data into a single dataframe. However, for my purposes 3 million is LUDICROUS and more than enough records for me to effectively analyze star classification distributions here's the math:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f8fea9",
   "metadata": {},
   "source": [
    "### random_index\n",
    "\"random_index\" was utilized to ensure that the entire collection of observed stars was to achieve an accurate representation of the population of observed stars,  while not downloading all datapoints in the Gaia DR3 dataset. See below for Gaia's explanation of this feature from their DR3 main source catalogue (https://gea.esac.esa.int/archive/documentation/GDR3/Gaia_archive/chap_datamodel/sec_dm_main_source_catalogue/ssec_dm_gaia_source.html)\n",
    "\n",
    "\"\n",
    "\n",
    "    random_index : Random index for use when selecting subsets (long)\n",
    "\n",
    "    A random index which can be used to select smaller subsets of the data that are still representative. The column contains a random permutation of the numbers from 0 to N−1, where N is the number of sources in the table.\n",
    "\n",
    "    The random index can be useful for validation (testing on 10 different random subsets), visualization (displaying 1% of the data), and statistical exploration of the data, without the need to download all the data.\n",
    "    \n",
    "\"\n",
    "\n",
    "### paralax > 0 (preemtive cleaning procedure)\n",
    "Please see the README documentation on an explanation of parallax if you are unfamiliar.\n",
    "In a perfect world negative parallaxes should NOT be measured. However, a large sum of stars are measured of having a negative parallax as a consequence of the incredible distance of these stars from Earth. Measurement errors of this nature appear from large distances when Gaia is unable to measure parallax precisely. Removing datapoints with negative parallaxes ensures that stars with unreliable and uniformative distance measurements are not factored into our analysis. This is a practical cut to our data to ensure the quality and integrity of our data is maintained."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
